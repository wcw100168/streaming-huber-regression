"""
ä½¿ç”¨ YearPredictionMSD è³‡æ–™é›†çš„å®Œæ•´æ•´åˆæ¸¬è©¦

é€™å€‹æ¸¬è©¦ä½¿ç”¨çœŸå¯¦çš„ YearPredictionMSD è³‡æ–™é›†ä¾†é©—è­‰ä¸²æµ Huber å›æ­¸æ¨¡å‹çš„å®Œæ•´åŠŸèƒ½ã€‚
æ¸¬è©¦å…§å®¹åŒ…æ‹¬ï¼š
1. è³‡æ–™è¼‰å…¥å’Œé è™•ç†
2. ä¸²æµè¨“ç·´éç¨‹
3. æ¨¡å‹é æ¸¬å’Œè©•ä¼°
4. æ­£å‰‡åŒ–å’Œéæ­£å‰‡åŒ–ç‰ˆæœ¬çš„æ¯”è¼ƒ
"""

import os
import sys
import time
import numpy as np
from typing import Tuple, Dict, Any, List

# æ·»åŠ å¥—ä»¶è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from streaming_huber.data.loader import StreamingDataLoaderWithDask
from streaming_huber.core.trainer import StreamingHuberModelTrainer
from streaming_huber.utils.training import streaming_huber_training


class TestYearPredictionMSDIntegration:
    """YearPredictionMSD è³‡æ–™é›†æ•´åˆæ¸¬è©¦"""
    
    @classmethod
    def setup_class(cls):
        """é¡åˆ¥åˆå§‹åŒ–è¨­ç½®"""
        cls.data_file_path = os.path.join(os.path.dirname(__file__), '../../data/YearPredictionMSD.csv')
        cls.n_features = 90  # YearPredictionMSD æœ‰ 90 å€‹ç‰¹å¾µ
        
        # æ¸¬è©¦åƒæ•¸
        cls.train_samples = 50000  # æ¸›å°‘æ¨£æœ¬æ•¸ä»¥åŠ å¿«æ¸¬è©¦
        cls.batch_size = 1000
        cls.n_batch = 50
        
        print(f"åˆå§‹åŒ– YearPredictionMSD æ•´åˆæ¸¬è©¦")
        print(f"è³‡æ–™æª”æ¡ˆ: {cls.data_file_path}")
        print(f"è¨“ç·´æ¨£æœ¬æ•¸: {cls.train_samples:,}")
        print(f"æ‰¹æ¬¡å¤§å°: {cls.batch_size}")
        print(f"æ‰¹æ¬¡æ•¸é‡: {cls.n_batch}")
    
    def verify_data_file(self) -> bool:
        """é©—è­‰è³‡æ–™æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¢º"""
        if not os.path.exists(self.data_file_path):
            print(f"éŒ¯èª¤: è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {self.data_file_path}")
            return False
        
        try:
            # å˜—è©¦è¼‰å…¥ä¸€å°éƒ¨åˆ†è³‡æ–™ä¾†é©—è­‰æ ¼å¼
            import pandas as pd
            sample_data = pd.read_csv(self.data_file_path, header=None, nrows=10)
            
            if sample_data.shape[1] != self.n_features + 1:  # +1 for target
                print(f"éŒ¯èª¤: è³‡æ–™ç¶­åº¦ä¸æ­£ç¢ºã€‚æœŸæœ› {self.n_features + 1} åˆ—ï¼Œå¯¦éš› {sample_data.shape[1]} åˆ—")
                return False
            
            print(f"âœ“ è³‡æ–™æª”æ¡ˆé©—è­‰é€šéï¼Œå½¢ç‹€: {sample_data.shape}")
            return True
            
        except Exception as e:
            print(f"éŒ¯èª¤: ç„¡æ³•è¼‰å…¥è³‡æ–™æª”æ¡ˆ: {e}")
            return False
    
    def test_data_loader(self) -> bool:
        """æ¸¬è©¦è³‡æ–™è¼‰å…¥å™¨åŠŸèƒ½"""
        print("\n=== æ¸¬è©¦è³‡æ–™è¼‰å…¥å™¨ ===")
        
        try:
            loader = StreamingDataLoaderWithDask(
                self.data_file_path,
                self.batch_size,
                self.train_samples
            )
            
            # æ¸¬è©¦ç²å–æ‰¹æ¬¡
            batch_count = 0
            total_samples = 0
            
            while batch_count < 5:  # åªæ¸¬è©¦å‰ 5 å€‹æ‰¹æ¬¡
                X_batch, y_batch = loader.get_batch()
                if X_batch is None:
                    break
                
                batch_count += 1
                total_samples += len(X_batch)
                
                # é©—è­‰æ‰¹æ¬¡å½¢ç‹€
                assert X_batch.shape[1] == self.n_features, f"ç‰¹å¾µç¶­åº¦ä¸æ­£ç¢º: {X_batch.shape[1]}"
                assert len(y_batch) == len(X_batch), "ç‰¹å¾µå’Œæ¨™ç±¤æ•¸é‡ä¸åŒ¹é…"
                
                print(f"æ‰¹æ¬¡ {batch_count}: {X_batch.shape}, æ¨™ç±¤ç¯„åœ: [{y_batch.min():.1f}, {y_batch.max():.1f}]")
            
            # æ¸¬è©¦çµ±è¨ˆä¿¡æ¯
            stats = loader.get_loader_stats()
            print(f"è¼‰å…¥å™¨çµ±è¨ˆ: {stats}")
            
            # æ¸¬è©¦æ¸¬è©¦è³‡æ–™è¼‰å…¥
            X_test, y_test = loader.get_test_data()
            if X_test is not None:
                print(f"æ¸¬è©¦è³‡æ–™è¼‰å…¥æˆåŠŸ: {X_test.shape}")
            
            loader.close()
            print("âœ“ è³‡æ–™è¼‰å…¥å™¨æ¸¬è©¦é€šé")
            return True
            
        except Exception as e:
            print(f"âœ— è³‡æ–™è¼‰å…¥å™¨æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_streaming_training_no_penalty(self) -> Tuple[bool, Dict[str, Any]]:
        """æ¸¬è©¦ä¸å¸¶æ­£å‰‡åŒ–çš„ä¸²æµè¨“ç·´"""
        print("\n=== æ¸¬è©¦ä¸²æµè¨“ç·´ï¼ˆç„¡æ­£å‰‡åŒ–ï¼‰===")
        
        try:
            # å»ºç«‹è³‡æ–™è¼‰å…¥å™¨
            loader = StreamingDataLoaderWithDask(
                self.data_file_path,
                self.batch_size,
                self.train_samples
            )
            
            start_time = time.time()
            
            # åŸ·è¡Œè¨“ç·´
            trainer, results = streaming_huber_training(
                data_loader=loader,
                n_features=self.n_features,
                n_batch=self.n_batch,
                penalty=False,
                tau_estimation='adaptive',
                mode='fast',
                verbose=True
            )
            
            training_time = time.time() - start_time
            
            # é©—è­‰çµæœ
            assert len(results) == self.n_batch, f"çµæœæ•¸é‡ä¸æ­£ç¢º: {len(results)}"
            assert trainer.is_initialized, "æ¨¡å‹æœªæ­£ç¢ºåˆå§‹åŒ–"
            assert trainer.beta is not None, "æ¨¡å‹åƒæ•¸æœªè¨­å®š"
            assert len(trainer.beta) == self.n_features + 1, "åƒæ•¸ç¶­åº¦ä¸æ­£ç¢º"
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            mae_history = [r['mae'] for r in results]
            avg_mae = np.mean(mae_history)
            final_mae = mae_history[-1]
            mae_improvement = mae_history[0] - mae_history[-1]
            
            test_results = {
                'avg_mae': avg_mae,
                'final_mae': final_mae,
                'mae_improvement': mae_improvement,
                'training_time': training_time,
                'sparsity': results[-1]['sparsity'],
                'model_state': trainer.get_model_state()
            }
            
            print(f"âœ“ ç„¡æ­£å‰‡åŒ–è¨“ç·´å®Œæˆ:")
            print(f"  - å¹³å‡ MAE: {avg_mae:.6f}")
            print(f"  - æœ€çµ‚ MAE: {final_mae:.6f}")
            print(f"  - MAE æ”¹å–„: {mae_improvement:.6f}")
            print(f"  - è¨“ç·´æ™‚é–“: {training_time:.2f} ç§’")
            print(f"  - ä¿‚æ•¸éé›¶é …: {test_results['sparsity']}")
            
            return True, test_results
            
        except Exception as e:
            print(f"âœ— ç„¡æ­£å‰‡åŒ–è¨“ç·´æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False, {}
    
    def test_streaming_training_with_penalty(self) -> Tuple[bool, Dict[str, Any]]:
        """æ¸¬è©¦å¸¶æ­£å‰‡åŒ–çš„ä¸²æµè¨“ç·´"""
        print("\n=== æ¸¬è©¦ä¸²æµè¨“ç·´ï¼ˆå¸¶æ­£å‰‡åŒ–ï¼‰===")
        
        try:
            # å»ºç«‹è³‡æ–™è¼‰å…¥å™¨
            loader = StreamingDataLoaderWithDask(
                self.data_file_path,
                self.batch_size,
                self.train_samples
            )
            
            start_time = time.time()
            
            # åŸ·è¡Œè¨“ç·´
            trainer, results = streaming_huber_training(
                data_loader=loader,
                n_features=self.n_features,
                n_batch=self.n_batch,
                penalty=True,
                auto_lambda=True,
                tau_estimation='adaptive',
                mode='fast',
                verbose=True
            )
            
            training_time = time.time() - start_time
            
            # é©—è­‰çµæœ
            assert len(results) == self.n_batch, f"çµæœæ•¸é‡ä¸æ­£ç¢º: {len(results)}"
            assert trainer.is_initialized, "æ¨¡å‹æœªæ­£ç¢ºåˆå§‹åŒ–"
            assert trainer.beta is not None, "æ¨¡å‹åƒæ•¸æœªè¨­å®š"
            assert len(trainer.beta) == self.n_features + 1, "åƒæ•¸ç¶­åº¦ä¸æ­£ç¢º"
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            mae_history = [r['mae'] for r in results]
            sparsity_history = [r['sparsity'] for r in results]
            avg_mae = np.mean(mae_history)
            final_mae = mae_history[-1]
            final_sparsity = sparsity_history[-1]
            mae_improvement = mae_history[0] - mae_history[-1]
            
            test_results = {
                'avg_mae': avg_mae,
                'final_mae': final_mae,
                'mae_improvement': mae_improvement,
                'training_time': training_time,
                'sparsity': final_sparsity,
                'sparsity_history': sparsity_history,
                'model_state': trainer.get_model_state()
            }
            
            print(f"âœ“ æ­£å‰‡åŒ–è¨“ç·´å®Œæˆ:")
            print(f"  - å¹³å‡ MAE: {avg_mae:.6f}")
            print(f"  - æœ€çµ‚ MAE: {final_mae:.6f}")
            print(f"  - MAE æ”¹å–„: {mae_improvement:.6f}")
            print(f"  - è¨“ç·´æ™‚é–“: {training_time:.2f} ç§’")
            print(f"  - æœ€çµ‚ä¿‚æ•¸éé›¶é …: {final_sparsity}")
            print(f"  - ä¿‚æ•¸éé›¶é …ç¯„åœ: [{min(sparsity_history)}, {max(sparsity_history)}]")
            
            return True, test_results
            
        except Exception as e:
            print(f"âœ— æ­£å‰‡åŒ–è¨“ç·´æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False, {}
    
    def test_prediction_and_evaluation(self) -> bool:
        """æ¸¬è©¦æ¨¡å‹é æ¸¬å’Œè©•ä¼°"""
        print("\n=== æ¸¬è©¦æ¨¡å‹é æ¸¬å’Œè©•ä¼° ===")
        
        try:
            # è¨“ç·´æ¨¡å‹
            loader = StreamingDataLoaderWithDask(
                self.data_file_path,
                self.batch_size,
                self.train_samples
            )
            
            trainer, _ = streaming_huber_training(
                data_loader=loader,
                n_features=self.n_features,
                n_batch=20,  # è¼ƒå°‘æ‰¹æ¬¡ä»¥åŠ å¿«æ¸¬è©¦
                penalty=True,
                mode='fast',
                verbose=False
            )
            
            # ç²å–æ¸¬è©¦è³‡æ–™
            test_loader = StreamingDataLoaderWithDask(
                self.data_file_path,
                self.batch_size,
                self.train_samples
            )
            
            X_test, y_test = test_loader.get_test_data()
            test_loader.close()
            
            if X_test is None or y_test is None:
                print("âš  ç„¡æ³•è¼‰å…¥æ¸¬è©¦è³‡æ–™ï¼Œè·³éé æ¸¬æ¸¬è©¦")
                return True
            
            # é™åˆ¶æ¸¬è©¦è³‡æ–™å¤§å°ä»¥åŠ å¿«æ¸¬è©¦
            if len(X_test) > 5000:
                X_test = X_test[:5000]
                y_test = y_test[:5000]
            
            # é€²è¡Œé æ¸¬
            y_pred = trainer.predict(X_test)
            
            # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
            mae = np.mean(np.abs(y_test - y_pred))
            rmse = np.sqrt(np.mean((y_test - y_pred)**2))
            mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            
            # è¨ˆç®—ç›¸é—œä¿‚æ•¸
            correlation = np.corrcoef(y_test, y_pred)[0, 1]
            
            print(f"âœ“ é æ¸¬è©•ä¼°å®Œæˆ:")
            print(f"  - æ¸¬è©¦æ¨£æœ¬æ•¸: {len(X_test):,}")
            print(f"  - MAE: {mae:.6f}")
            print(f"  - RMSE: {rmse:.6f}")
            print(f"  - MAPE: {mape:.2f}%")
            print(f"  - ç›¸é—œä¿‚æ•¸: {correlation:.4f}")
            print(f"  - é æ¸¬å€¼ç¯„åœ: [{y_pred.min():.1f}, {y_pred.max():.1f}]")
            print(f"  - çœŸå¯¦å€¼ç¯„åœ: [{y_test.min():.1f}, {y_test.max():.1f}]")
            
            # åŸºæœ¬åˆç†æ€§æª¢æŸ¥
            assert np.all(np.isfinite(y_pred)), "é æ¸¬å€¼åŒ…å«éæœ‰é™å€¼"
            assert mae > 0, "MAE æ‡‰è©²å¤§æ–¼ 0"
            assert correlation > 0, "ç›¸é—œä¿‚æ•¸æ‡‰è©²ç‚ºæ­£"
            
            return True
            
        except Exception as e:
            print(f"âœ— é æ¸¬è©•ä¼°æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_model_persistence(self) -> bool:
        """æ¸¬è©¦æ¨¡å‹ç‹€æ…‹çš„ä¿å­˜å’Œæ¢å¾©"""
        print("\n=== æ¸¬è©¦æ¨¡å‹æŒä¹…åŒ– ===")
        
        try:
            # è¨“ç·´ä¸€å€‹å°æ¨¡å‹
            loader = StreamingDataLoaderWithDask(
                self.data_file_path,
                self.batch_size,
                self.train_samples
            )
            
            trainer, _ = streaming_huber_training(
                data_loader=loader,
                n_features=self.n_features,
                n_batch=10,
                penalty=False,
                verbose=False
            )
            
            # ç²å–æ¨¡å‹ç‹€æ…‹
            model_state = trainer.get_model_state()
            
            # é©—è­‰ç‹€æ…‹å®Œæ•´æ€§
            required_keys = ['beta', 'J_cumulative', 'tau', 'batch_count', 
                           'mae_history', 'is_initialized', 'standardizer_stats']
            
            for key in required_keys:
                assert key in model_state, f"æ¨¡å‹ç‹€æ…‹ç¼ºå°‘éµ: {key}"
            
            # é©—è­‰é—œéµæ•¸å€¼
            assert model_state['beta'] is not None, "beta åƒæ•¸ä¸æ‡‰ç‚º None"
            assert len(model_state['beta']) == self.n_features + 1, "beta ç¶­åº¦ä¸æ­£ç¢º"
            assert model_state['batch_count'] == 10, "æ‰¹æ¬¡è¨ˆæ•¸ä¸æ­£ç¢º"
            assert model_state['is_initialized'] == True, "åˆå§‹åŒ–ç‹€æ…‹ä¸æ­£ç¢º"
            assert len(model_state['mae_history']) == 10, "MAE æ­·å²é•·åº¦ä¸æ­£ç¢º"
            
            print(f"âœ“ æ¨¡å‹æŒä¹…åŒ–æ¸¬è©¦é€šé:")
            print(f"  - Beta ç¶­åº¦: {len(model_state['beta'])}")
            print(f"  - æ‰¹æ¬¡è¨ˆæ•¸: {model_state['batch_count']}")
            print(f"  - Tau å€¼: {model_state['tau']:.6f}")
            print(f"  - æ¨™æº–åŒ–æ¨£æœ¬æ•¸: {trainer.standardizer.count:,}")
            
            return True
            
        except Exception as e:
            print(f"âœ— æ¨¡å‹æŒä¹…åŒ–æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_all_tests(self) -> Dict[str, bool]:
        """é‹è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("="*60)
        print("é–‹å§‹ YearPredictionMSD æ•´åˆæ¸¬è©¦")
        print("="*60)
        
        results = {}
        
        # 1. é©—è­‰è³‡æ–™æª”æ¡ˆ
        results['data_file_verification'] = self.verify_data_file()
        if not results['data_file_verification']:
            print("è³‡æ–™æª”æ¡ˆé©—è­‰å¤±æ•—ï¼Œåœæ­¢æ¸¬è©¦")
            return results
        
        # 2. æ¸¬è©¦è³‡æ–™è¼‰å…¥å™¨
        results['data_loader'] = self.test_data_loader()
        
        # 3. æ¸¬è©¦ç„¡æ­£å‰‡åŒ–è¨“ç·´
        success, no_penalty_results = self.test_streaming_training_no_penalty()
        results['training_no_penalty'] = success
        
        # 4. æ¸¬è©¦æ­£å‰‡åŒ–è¨“ç·´
        success, penalty_results = self.test_streaming_training_with_penalty()
        results['training_with_penalty'] = success
        
        # 5. æ¸¬è©¦é æ¸¬å’Œè©•ä¼°
        results['prediction_evaluation'] = self.test_prediction_and_evaluation()
        
        # 6. æ¸¬è©¦æ¨¡å‹æŒä¹…åŒ–
        results['model_persistence'] = self.test_model_persistence()
        
        # ç¸½çµæ¸¬è©¦çµæœ
        print("\n" + "="*60)
        print("æ¸¬è©¦çµæœç¸½çµ")
        print("="*60)
        
        passed_tests = sum(results.values())
        total_tests = len(results)
        
        for test_name, passed in results.items():
            status = "âœ“ é€šé" if passed else "âœ— å¤±æ•—"
            print(f"{test_name:.<30} {status}")
        
        print("-"*60)
        print(f"ç¸½è¨ˆ: {passed_tests}/{total_tests} æ¸¬è©¦é€šé")
        
        if passed_tests == total_tests:
            print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼å¥—ä»¶åŠŸèƒ½æ­£å¸¸")
        else:
            print("âš  éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦æª¢æŸ¥å•é¡Œ")
        
        return results


def main():
    """ä¸»å‡½æ•¸"""
    # è¨­å®šè³‡æ–™æª”æ¡ˆè·¯å¾‘
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_file_path = os.path.join(current_dir, '..', '..', '..', 'data', 'YearPredictionMSD.csv')
    
    # æª¢æŸ¥ç›¸å°è·¯å¾‘
    if not os.path.exists(data_file_path):
        # å˜—è©¦çµ•å°è·¯å¾‘
        data_file_path = "/Users/user/Downloads/å·¨é‡è³‡æ–™åˆ†ææœŸæœ«/å¯¦ä½œå˜—è©¦v7/data/YearPredictionMSD.csv"
    
    if not os.path.exists(data_file_path):
        print("éŒ¯èª¤: æ‰¾ä¸åˆ° YearPredictionMSD.csv æª”æ¡ˆ")
        print("è«‹ç¢ºä¿è³‡æ–™æª”æ¡ˆä½æ–¼æ­£ç¢ºçš„è·¯å¾‘")
        return
    
    # é‹è¡Œæ¸¬è©¦
    test_suite = TestYearPredictionMSDIntegration(data_file_path)
    results = test_suite.run_all_tests()
    
    return results


if __name__ == "__main__":
    main()
